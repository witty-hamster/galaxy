[TOC]

# 0. 认知Kafka

- 官方定义：
  - A distributed streaming platform （一个分布式流平台）
- 我们可以简单理解
  - Kafka 是基于 Zookeeper 的分布式消息系统
- 特点:
  - Kafka 具有高吞吐量、高性能、实时及高可靠等特点

# 1. 发布与订阅消息系统

- 发布与订阅消息系统的一大特点
    - 消息的发送者（发布者）不会直接把消息发送给接收者
- 发布与订阅系统一般会有一个 broker，作为发布消息的中心点

# 2. Kafka 常用术语

## 2.1 消息 & 批次
- 消息：Kafka 的数据单元
    - 消息由字节数组组成，对kafka来说，消息内容没有特别的格式与含义
- 批次：为了提高效率，减少网络开销，消息可分批写入Kafka
    - 批次是一组消息，这一组消息属于同一个主题和分区

## 2.2 模式
- 各类消息模式
    - JSON
    - XML
    - Apache Avro 序列化格式

## 2.3 主题 Topic & 分区 Partition
- 主题 Topic
    - 一个虚拟的概念，由 1 个到多个 Partitions 组成
    - 好比数据库的表
    
- 分区 Partition
    - 实际消息存储单位
    - 主题可以被分为若干个分区，一个分区就是一个提交日志


## 2.4 生产者 Producer & 消费者 Consumer
- 生产者：创建消息
    - 又称：发布者、写入者
    - 在默认情况下，生产者把消息均衡的分布到主题的所有分区上，而并不关心特定消息会被写到哪个分区
    - 在某些情况下，生产者可以通过消息键和分区器来实现，将消息写到特定分区
    - 生产者可自定义特定的分区器
- 消费者：读取消息
    - 又称：订阅者、读者
    - 消费者订阅一个或多个主题，并按照消息生产的顺序读取它们
    - 通过检查消息的<b style="color:#ff0000">偏移量</b>来区分读取过的消息
      - <b style="color:#ff0000">偏移量</b> 是一种元数据，它是一个不断递增的整数值，在创建消息时，Kafka 会把它添加到消息里
      - 在每个分区中，每个消息的偏移量都是唯一的。消费者会将其保存在 Zookeeper或者Kafka上

- 消费者组
  - 消费者群组保证每个分区只能被一个消费者使用
  - 消费者与分区之间的映射通常被称为消费者对分区的所有权关系

## 2.5 broker & 集群

- broker

  - 一个独立的Kafka服务器被称为 broker
  - 功能
    - 接收生产者所发来的消息
    - 为消息设置偏移量
    - 提交消息到磁盘保存
    - 为消费者提供服务
    - 对读取分区的请求做出响应
    - 返回已经提交到磁盘上的消息

- 集群

  - 多个 broker 构成集群
  - 每个集群都有一个 broker 同时充当了 **集群控制器** 的角色（通过选举）
  - 控制器的工作
    - 负责集群中节点的管理工作
    - 将哪些分区分给哪些对应的 broker
    - 监控 broker
    - 等

- 简单集群模型示例图

  ![简单集群模型示例图](http://witty-hamster.gitee.io/draw-bed/Kafka/核心-简单集群模型示例图.png)

- 保留消息

  - 在一定期限内，Kafka可保留消息
  - 默认的消息保留策略
    - 保留一段时间（比如 7 天）
    - 保留到消息到达一定大小的字节数（比如 1GB）
  - 当消息数量达到保留消息策略的上限时，旧的消息就会被过期删除

# 3. 为什么选择 Kafka

- 多个生产者

- 多个消费者
- 基于磁盘的数据存储
  - 允许消费者非实时地读取消息，因为Kafka有保留消息的特性
  - 每个主题可以设置单独的保留规则
- 伸缩性
- 高性能

# 4. Broker 配置

## 4.1 常规配置

> 有一些配置选项，在单机安装时可以直接使用默认值，但在部署到其他环境时要格外小心。

### `broker.id`

- 每个 broker 都需要有一个标识符，使用 `broker.id` 表示。
- 默认值是 `0`，也可以被设置为其他任意整数
- 该值在Kafka集群中，必须要唯一

### `port`

- 默认的端口号是 `9092`

### `zookeeper.connect`

- 用于保存 broker 元数据的 Zookeeper 地址，是通过该参数进行指定
- `localhost:2181` 表示这个 Zookeeper 是运行在本地的 `2181` 端口上
- 该配置参数是用分号分隔的一组 `hostname:port/path` 列表，每部分的含义如下：
  - `hostname`：是 Zookeeper 服务器的机器名或 IP 地址
  - `port`：是 Zookeeper 的客户端连接端口
  - `/path`：是可选的 Zookeeper 路径，作为 Kafka 集群的 chroot 环境。
    - 如果不指定，默认使用根路径
    - 如果指定的 chroot 路径不存在，broker 会在启动的时候创建它

### `log.dir`

- Kafka 把所有消息都保存在磁盘上，存放这些日志片段的目录是通过该参数指定的
- 它是一组用逗号分隔的本地文件系统路径
  - 如果指定了多个路径，那么 broker 会根据 "最少使用" 原则，把同一分区的日志片段保存到同一个路径下

### `num.recovery.threads.per.data.dir`

- 有以下 3 种情况，Kafka可通过配置线程池来处理日志片段
  - 服务器正常启动，用于打开每个分区的日志片段
  - 服务器崩溃后重启，用于检查和截短每个分区的日志片段
  - 服务器正常关闭，用于关闭日志片段
- 注意：所配置的数字对应的是 `log.dir` 指定的单个日志目录
  - 例如：`num.recovery.threads.per.data.dir` 设置为 8 ，并且 `log.dir` 设置了 3 个路径，那么总共需要 24 个线程

### `auto.create.topics.enable`

- 默认情况下，Kafka会对如下 3 种情况自动创建主题
  1. 当一个生产者开始往主题写入消息时
  2. 当一个消费者开始从主题读取消息时
  3. 当任意一个客户端向主题发送元数据请求时
- 根据 Kafka 协议，如果一个主题不先被创建，那么根本无法知道它是否已经存在
- 如果显示的创建主题，不管手动还是通过配置系统创建，都可以把 `auto.create.topics.enable` 设置为 false

## 4.2 主题的默认配置

> 通过管理工具手段，可以为每个主题配置特定参数。此时，服务器提供的默认配置将作为基准参数

### `num.partitions`

- 指定了新创建的主题将包含多少个分区
- 默认值：1
- <b style="color:#ff0000">注意：可以增加主题分区的个数，但不能减少分区个数</b>

### `log.retention.ms`

- 配置Kafka决定数据可以保留多长时间
- 默认值：168 小时（一周）
- 功能与该参数一样的另外两个参数，仅仅是时间单位不同
  - `log.retention.minutes`
  - `log.retention.s`
- 推荐使用以 `ms` 结尾的配置项，Kafka会优先使用具有最小值的参数项

### `log.retention.bytes`

- 指定通过保留的消息字节数来判断消息是否过期
- 作用范围：每一个分区上

### `log.segment.bytes`

- 以上的设置都是作用于日志片段上的，该参数是作用于消息上的
- 默认值：1GB
- 作用：
  - 当消息到达 broker时，它们被迫加到分区的当前日志片段上。当日志片段大小达到 `log.segment.bytes` 指定的上限时，当前日志片段就会被关闭，一个新的日志片段被打开

### `log.segment.ms`

- 另一个控制日志片段关闭时间的参数
- 指定了多长时间之后日志片段会被关闭
- 默认情况下，`log.segment.ms` 没有设定值

### `message.max.bytes`2.6.1

- 限制单个消息的大小（压缩后的消息的大小）
- 默认值：1000000（1MB）
- 该参数值对性能有显著的影响。值越大，那么负责处理网络连接和请求的线程就需要花费越多的时间来处理请求。还会增加磁盘写入块的大小，从而影响IO吞吐量



# 5. 五大类 Kafka API

- `AdminClient` API
  - 允许管理和检测 Topic、broker 以及其他 Kafka 对象
- `Prodicer` API
  - 发布消息到一个或多个 topic
- `Consumer` API
  - 订阅一个或多个 topic，并处理产生的消息
- `Streams` API
  - 高效地将输入流转换到输出流
- `Connector` API
  - 从一些源系统或应用程序中拉取数据到 Kafka（Kafka 与 DB数据库之间的交互）



# TIPS

## 1. 如何选择分区数量？

- 在进行数量选择时，需要考虑如下几个因素
  1. 主题需要达到多大的吞吐量？
  2. 从单个分区读取数据的最大吞吐量是多少？
  3. 可以通过类似的方法估算生产者向单个分区写入数据的吞吐量，不过生产者的速度一般比消费者快得多， 所以最好为生产者多估算一些吞吐量
  4. 每个 broker 包含的分区个数、可用的磁盘空间和网络带宽
  5. 如果消息是按照不同的键来写入分区的，那么为已有的主题新增分区就会很困难
  6. 单个 broker 对分区个数是有限制的，因为分区越多，占用的内存越多，完成首领选举需要的时间也越长
- 如果你能估算出主题的吞吐量和消费者的吞吐量，可以用主题吞吐量除以消费者吞吐量算出分区个数
  - 例如：如果每秒要从主题上写入和读取 1GB 的数据，而每个消费者在每秒钟可以处理 50MB 的数据，那么至少需要 20 个分区。
- 如果你不知道这些信息，那么根据经验来看，通常把分区大小限制在 25GB 以内可以得到比较理想的效果

## 2. 消息时效性保证？

- 根据字节大小和时间保留数据’
  - 如果同时指定了 `log.retention.bytes` 和 `log.retention.ms` ，只要任意一个条件得到满足，消息就会被删除。

## 3. 如何调整日志片段的滚动策略，提高磁盘写入的整体效率？

- 涉及到的配置参数
  - `log.segment.bytes`
  - `log.segment.ms`
- 如果一个主题每天只接收 100MB 的消息，而 `log.segment.bytes` 使用默认设置（1GB），那么需要 10天时间才能填满一个日志片段。因为在日志片段被关闭之前，消息是不会过期的，所以，如果 `log.retention.ms` 被设置为 604800000（1周），那么日志片段最多需要 17天才会过期。这是因为关闭日志片段需要 10天的时间，而根据配置的过期时间，还需要再保留 7天时间。

